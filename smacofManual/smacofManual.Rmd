---
title: |
    | Smacof at 50
    | A Manual
author: 
- Jan de Leeuw - University of California Los Angeles
date: '`r paste("Started February 21 2024, Version of",format(Sys.Date(),"%B %d, %Y"))`'
output:
  bookdown::pdf_document2:
    latex_engine: lualatex
    includes:
      in_header: preamble.tex
    keep_tex: yes
    toc: true
    toc_depth: 4
    number_sections: yes
  bookdown::html_document2:
    keep_md: yes
    css: preamble.css
    toc: true
    toc_depth: 4
    number_sections: yes
graphics: yes
mainfont: Times New Roman
fontsize: 12pt
bibliography: ["mypubs.bib","total.bib"]
abstract: TBD
editor_options: 
  markdown: 
    wrap: 72
---

```{r loadpackages, echo = FALSE}
suppressPackageStartupMessages(library(splines, quietly = TRUE))
suppressPackageStartupMessages(library(knitr, quietly = TRUE))
suppressPackageStartupMessages(library(tinytex, quietly = TRUE))
```

```{r load code, echo = FALSE}
#source("rcode/smacofSort.R")
```

**Note:** This is a working paper which will be expanded/updated
frequently. All suggestions for improvement are welcome. All Rmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. The files can be found at
<https://github.com/deleeuw/cSmacof>.


# Introduction

In Multidimensional Scaling (MDS) the data are
dissimilarities between pairs of elements selected from
a set of $n$ objects $\mathcal{O}=\{o_1,\cdots,o_n\}$. In 
Metric MDS we have numerical dissimilarity measures and we want to map
the objects $o_i$ into $n$ points $x_i$ in some metric space in such a way
that the distances between the points approximate the dissimilarities
between the objects. In p-dimensional Euclidean Metric MDS the metric space is $\mathbb{R}^p$, the space of all $p$-tuples of real numbers, with the usual Euclidean distance.

In the pioneering papers @kruskal_64a and @kruskal_64b the MDS problem
was formulated for the first time as minimization of an explicit loss
function, which measures the quality of the approximation of the
dissimilarities by the distances. The loss function in Least Squares Metric Euclidean MDS
is \begin{equation}
\sigma(X):=\frac12\jis w_{ij}(\delta_{ij}-d_{ij}(X))^2.
(\#eq:stressdef)
\end{equation}
The symbol $:=$ is used for definitions. In definition
\@ref(eq:stressdef) the $w_{ij}$ are known non-negative *weights*, the $\delta_{ij}$ are the known
non-negative *dissimilarities* between objects $o_i$ and $o_j$, and the
$d_{ij}(X)$ are the *distances* between the corresponding points $x_i$
and $x_j$. From now on we use "metric MDS" to 
mean Least Squares Metric Euclidean MDS.

The $n\times p$ matrix $X$, which has the coordinates $x_i$
of the $n$ points as its rows, is called the *configuration*, where
$p$ is the *dimension* of the Euclidean space in which we make the
map. Thus
\begin{equation}
d_{ij}(X)=\sqrt{\sum_{s=1}^p(x_{is}-x_{js})^2}.
(\#eq:ddef)
\end{equation}
The metric MDS problem (of dimension $p$, for given $W$ and $\Delta$
is the minimization of \@ref(eq:stressdef) over the $n\times p$
configurations $X$.

The weights $w_{ij}$ can be used to quantify information about the 
precision or importance of the corresponding dissimilarities. Some of the weights may be zero, which can be used to code *missing data*. If all weights are positive we have *complete data*. If we have complete data, and all weights are equal to one, we have *unweighted* metric MDS. Weights were only introduced in MDS in @deleeuw_C_77, the pioneering papers by Shepard, Kruskal, and Guttman only consider the unweighted case.

We assume
throughout that the weights are *irreducible* (@deleeuw_C_77). This means there 
is no partitioning of the index set $I_n:=\{1,2,\cdots,n\}$
into subsets for which all between-subset weights are zero. A reducible 
metric MDS problems decomposes into a number of smaller independent 
metric MDS problems, so the irreducibility assumption causes no real
loss of generality. 

The fact that the summation in \@ref(eq:stressdef) is over all $j<i$
indicates that the diagonal elements of $\Delta$ are missing (they are
assumed to be zero) and the elements above the diagonal are missing as
well (they are assumed to be equal to the corresponding elements below
the diagonal). The factor $\frac12$ in definition
\@ref(eq:stressdef) is there because it simplifies some of the formulas in
later sections of this paper.

Kruskal was not primarily interested in
metric MDS and loss function \@ref(eq:stressdef). His papers are really about
non-metric MDS, by which we mean Least Squares Non-metric Euclidean MDS. Non-metric MDS differs
from metric MDS because we have incomplete information about the
dissimilarities. As we have seen, that if some dissimilarities are missing 
metric MDS can handle this by using zero weights. In some situations, however,
we only know the rank order of the non-missing dissimilarities. 
We do not know, or we refuse to use,
their actual numeric values. Or, to put it differently, even if we have numerical dissimilarities we are looking
for a *transformation* of the non-missing dissimilarities, where the transformation
is chosen from a set of admissible transformations (for instance
from all linear or monotone transformations). If the dissimilarities
are non-numerical, for example rank orders or partitionings, we
choose from the set of admissible *quantifications*.

In non-metric MDS the loss function becomes
\begin{equation}
\sigma(X,\hat D):=\frac12\jis w_{ij}(\hat d_{ij}-d_{ij}(X))^2,
(\#eq:rawstressdef)
\end{equation} 
where $\hat D$ are now the quantified or transformed dissimilarities. In
MDS parlance they are also called *pseudo-distances* or *disparities*. 
Loss function \@ref(eq:rawstressdef) must be minimized over both
configurations and disparities, with the condition that the disparities
$\hat D$ are an admissible transformation of the dissimilarities $\Delta$.
In Kruskal's non-metric MDS this means requiring monotonicity. In this paper 
we will consider various other choices for the set of admissible 
transformations. We will use the symbol $\mathfrak{D}$ for the set of
admissible transformations

Kruskal calls loss function \@ref(eq:rawstressdef) *raw stress*, which suggests
that the definition of the  non-metric MDS problem is not complete yet.
The most familiar sets of admissible transformations (linear, polynomial, monotone)
define convex cones with apex at the origin. Thus if $\hat D\in\mathfrak{D}$ then
so is $\lambda\hat D$ for all $\lambda\geq 0$. But this means that minimizing
\@ref(eq:rawstressdef) over all $\hat D\in\mathfrak{D}$ and over all
configurations has the trivial solution $\hat D=0$ and $X=0$, which gives
the trivial global minimum $\sigma(X,\hat D)=0$. We need additional constraints to rule out this trivial solution, and in non-metric MDS this is done by choosing a *normalization* that keeps the solution away from zero.

# Notation and Terminology {-}

We discuss some standard MDS notation, first introduced in @deleeuw_C_77. This
notation is useful for the second phase of the ALS algorithm, in which solve the metric MDS problem of we minimizing unnormalized
$\sigma(X,\hat D)$ over $X$ for fixed $\hat D$. We will discuss the first ALS phase
later in the paper.

Start with the unit vectors $e_i$ of length $n$. They have a non-zero
element equal to one in position $i$, all other elements are zero. Think
of the $e_i$ as the columns of the identity matrix.

Using the $e_i$ we define for all $i\not= j$ the matrices
\begin{equation}
A_{ij}:=(e_i-e_j)(e_i-e_j)'.
\end{equation} The $A_{ij}$ are of order $n$, symmetric, doubly-centered, and of rank one. They have four non-zero elements. Elements
$(i,i)$ and $(j,j)$ are equal to $+1$, elements $(i,j)$ and $(j,i)$ are
$-1$. 

The importance of $A_{ij}$ in MDS comes from the equation
\begin{equation}
d_{ij}^2(X)=\text{tr}\ X'A_{ij}X.
(\#eq:dfroma)
\end{equation}
In addition we use the fact that the $A_{ij}$ form a basis for the $binom{n}{2}$-dimensional linear space of all doubly-centered 
symmetric matrices.

Expanding the square in the definition of stress gives
\begin{equation}
\sigma(X)=\frac12\{\jis w_k\delta_k^2-2\ \jis w_k\delta_kd_k(X)+\jis w_kd_k^2(X)\}.
(\#eq:expand)
\end{equation}
It is convenient to have notation for the three separate
components of stress from equation \@ref(eq:expand). Define
\begin{align}
\eta_{\hat D}^2&=\jis w_{ij}\hat d_{ij}^2,(\#eq:condef)\\
\rho(X)&=\jis w_{ij}\hat d_{ij}d_{ij}(X),(\#eq:rhodef)\\
\eta^2(X)&=\jis w_{ij}d_{ij}(X)^2.(\#eq:etadef)
\end{align} which lead to 
\begin{equation}
\sigma(X)=\frac12\left\{\eta_{\hat D}^2-2\rho(X)+\eta^2(X)\right\}.
(\#eq:stressshort)
\end{equation}
We also need
\begin{equation}
\lambda(X)=\frac{\rho(X)}{\eta(X)}.
(\#eq:lambdadef)
\end{equation}

Using the $A_{ij}$ makes it possible to give matrix expressions for $\rho$
and $\eta^2$. First
\begin{equation}
\eta^2(X)=\text{tr}\ X'VX,
(\#eq:etamat)
\end{equation} 
with 
\begin{equation}
V:=\jis w_{ij}A_{ij}.
(\#eq:vdef)
\end{equation} 
In the same way
\begin{equation}
\rho(X)=\text{tr}\ X'B(X)X,
(\#eq:rhomat)
\end{equation}
with
\begin{equation}
B(X):=\jis w_{ij}\frac{\delta_{ij}}{d_{ij}(X)}A_{ij}.
(\#eq:bdef)
\end{equation}
Note that $B$ is a function from the set of $n\times p$ configurations into the set of symmetric doubly-dentered matrices of order $n$. Because $B(X)$ and $V$ are
non-negative linear combinations of the $A_{ij}$ they are both positive semi-definite.
Because $W$ is assumed to be irreducible the matrix $V$ has rank $n-1$, with only 
vectors proportional to the vector $e$ with all elements equal to one in its null-space (@deleeuw_C_77).


Summarizing the results so far we have 
\begin{equation}
\sigma(X)=\frac12\{\eta_{\hat D}^2-\text{tr}\ X'B(X)X+\text{tr}\ X'VX\}.
(\#eq:sigmat)
\end{equation}

Next we define the *Guttman transform* of a configuration $X$, for given $W$ and
$\Delta$, as 
\begin{equation}
G(X)=V^+B(X)X,
(\#eq:gudef)
\end{equation}
with $V^+$ the Moore-Penrose inverse of $V$. In our computations we use 
$$
V^+=(V+\frac{1}{n}ee')^{-1}-\frac{1}{n}ee'
$$ 
Also note that in the unweighted case with complete data
$V=nJ$, where $J$ is the  centering matrix $I-\frac{1}{n}ee'$, and thus
$V^+=\frac{1}{n}J$. The Guttman transform is then simply $G(X)=n^{-1}B(X)X$.


# Smacof Loss

## Normalization

Normalization in non-metric MDS has been discussed in detail in
@kruskal_carroll_69 and @deleeuw_U_75a. In the terminology of
@deleeuw_U_75a there are *explicit* and *implicit* normalizations.

In implicit normalization we minimize either 
\begin{equation}
\sigma(X,\hat D):=\frac{\jis w_{ij}(\hat d_{ij} -d_{ij}(X))^2}{\jis w_{ij}^{\ }\hat d_{ij}^2}
(\#eq:implicit1)
\end{equation} 
or 
\begin{equation}
\sigma(X,\hat D):=\frac{\jis w_{ij}(\hat d_{ij}-d_{ij}(X))^2}{\jis w_{ij}^{\ }d_{ij}^2(X) }
(\#eq:implicit2)
\end{equation} 
@kruskal_64a chooses definition \@ref(eq:implicit2) and calls the explicitly normalized
loss function *normalized stress*. In fact, he takes the square root, which 
does not change the minimization problem, and only considers the
unweighted case. Note that we overload the symbol $\sigma$ to denote any one
of the least squares loss functions. It will always be clear from the
text which $\sigma$ we are talking about.

In explicit normalization we minimize the raw stress $\sigma(X,\hat D)$
from \@ref(eq:rawstressdef), but we add the constraint 
\begin{equation}
\jis w_{ij}^{\ }d_{ij}^2(X)=1,
(\#eq:explicit1)
\end{equation} 
or the constraint 
\begin{equation}
\jis w_{ij}^{\ }\hat d_{ij}^2=1.
(\#eq:explicit2)
\end{equation} 
@kruskal_carroll_69 and @deleeuw_E_19d show that these
four normalizations all lead to essentially the same solution for $X$
and $\hat D$, up to scale factors dictated by the choice of
normalization. It is also possible to normalize both $X$ and $\hat D$,
either explicitly or implicitly, and again this will give the same
solutions, suitably normalized. These invariance results assume the
admissible transformations form a closed cone with apex at the origin,
i.e. if $\hat D$ is admissible and $\lambda\geq 0$ then $\lambda\hat D$
is admissible as well. The matrices of Euclidean distances $D(X)$ form a
similar closed cone as well. The LSNE-MDS problem is to find an element of the
$\hat D$ cone and an element of the $D(X)$ cone where the angle between
the two is a small as possible.

In the R version of smacof (@deleeuw_mair_A_09c,
@mair_groenen_deleeuw_A_22) we use explicit normalization
\@ref(eq:explicit2). This is supported by the result, also due to
@deleeuw_U_75a, that projection on the intersection of the cone of
disparities and the sphere defined by \@ref(eq:explicit2) is equivalent
to first projecting on the cone and then normalizing the projection (see
also @bauschke_bui_wang_18).

In our version of non-metric MDS we need more flexibility. For algorithmic
reasons that will become clear later on, we will go with the other explicit
normalization \@ref(eq:explicit1) and minimize $\sigma$ from
\@ref(eq:rawstressdef) over normalized $X$ and unnormalized $\hat D$.
For the final results the choice between \@ref(eq:explicit1) and 
\@ref(eq:explicit2) should not make a difference.

## Derivatives

### Lagrangian

### Gradient

$$
\mathcal{D}\sigma(X)=VX-B(X)X
$$



### Hessian

$$
H_{st}(X)=\jis w_{ij}\frac{\delta_{ij}}{d_{ij}^3(X)}(x_{is}-x_{js})(x_{it}-x_{jt})A_{ij}
$$
$$
\mathcal{D}_{st}\sigma(X)=\begin{cases}H_{st}(X)&\text{ if }s\not= t,\\
V-B(X)+H_{st}&\text{ if }s= t.
\end{cases}
$$
There are several ways to think of the Hessian. The simplest one
(perhaps) is as an $np\times np$ symmetric matrix (corresponding to
column-major R vector of length $\frac12 np(np+1)$). This is what we
would use for a straightforward version of Newton-Raphson.

It is more elegant, however, to think of $H$ as a symmetric super-matrix
of order $p$, with as elements $n\times n$ matrices. And, for some
purposes, such as the pseudo-confidence ellipsoids in @deleeuw_E_17q, as
a super-matrix of order $n$ with as elements $p\times p$ matrices. Both
the super-matrix interpretations lead to four-dimensional arrays, the
first a $p\times p\times n\times n$ array, the second an
$n\times n\times p\times p$ array. The different interpretations lead to
different ways to store the Hessian in memory, and to different ways to
retrieve its elements. Of course we can write routines to transform from
one interpretation to another.


## Stationary Points


# Smacof Algorithm

## Some thoughts on ALS

I will take this opportunity to clear up some misunderstandings and confusions 
that have haunted the early development of non-metric MDS. 

### The Single-Phase approach

In @kruskal_64a defines 
\begin{equation}
\sigma(X):=\min_{\hat D\in\mathfrak{D}}\ \sigma(\hat D,X)=\sigma(X,\hat D(X)),
(\#eq:project)
\end{equation}
where $\sigma(\hat D,X)$ is defined by \@ref(eq:implicit2).
where the minimum is over admissible transformations. In definition
\@ref(eq:project)
\begin{equation}
\hat D(X):=\mathop{\text{argmin}}_{\hat D\in\mathfrak{D}}\sigma(X, \hat D).
(\#eq:optscal)
\end{equation}
Normalized stress defined by \@ref(eq:project) is now a function of $X$ only. Under some
conditions, which are true in Kruskal's definition of non-metric MDS, 
\begin{equation}
\mathcal{D}\sigma(X)=\mathcal{D}_1\sigma(X,\hat D(X)),
(\#eq:partials)
\end{equation}
where $\mathcal{D}\sigma(X)$ are the derivatives of $\sigma$ from \@ref(eq:project) and $\mathcal{D}_1\sigma(X,\hat D(X))$ are the partial derivatives of $\sigma$ from \@ref(eq:implicit2) with respect to $X$. Thus the partials of $\sigma$ from \@ref(eq:project) can be computed by evaluating the partials of $\sigma$ from \@ref(eq:implicit2) with respect to $X$ at $(X,\hat D(X))$. This has created much confusion in the past. The non-metric MDS problem is now  to minimize $\sigma$ from \@ref(eq:project), which is a function of $X$ alone.

@guttman_68 calls this the *single-phase approach*. A variation of
Kruskal's single-phase approach defines 
$$
\sigma(X)=\jis w_{ij}(d_{ij}^\#(X)-d_{ij}(X))^2
$$ 
where the $d_{ij}^\#(X)$ are *Guttman's rank images*, i.e. the
permutation of the $d_{ij}(X)$ that makes them monotone with the
$\delta_{ij}$ (@guttman_68). Or, alternatively, define 
$$
\sigma(X):=\jis w_{ij}(d_{ij}^\%(X)-d_{ij}(X))^2
$$ 
where the $\hat d_{ij}^\%(X)$ are *Shepard's rank images*, i.e. the
permutation of the $\delta_{ij}$ that makes them monotone with the
$d_{ij}(X)$ (@shepard_62a, @shepard_62b, @deleeuw_E_17e).

Minimizing the Shepard and Guttman single-phase loss functions is
computationally more complicated than Kruskal's *monotone regression*
approach, mostly because the rank-image transformations are not differentiable,
and there is no analog of \@ref(eq:partials) and of the equivalence of 
the different implicit and explicit normalizations.

### The Two-Phase Approach

The *two-phase approach* or *alternating least squares (ALS)* approach
alternates minimization of $\sigma(\hat D,X)$ over $X$ for our current
best estimate of $\hat D$ with minimization of $\sigma(\hat D,X)$ over
$\Delta\in\mathfrak{D}$ for our current best value of $X$. Thus an
update from iteration $k$ to iteration $k+1$ looks like 
\begin{align}
\hat D^{(k)}&=\mathop{\text{argmin}}_{\hat D\in\mathfrak{D}}\sigma(\hat D,X^{(k)}),(\#eq:step1)\\
X^{(k+1)}&=\mathop{\text{argmin}}_X\sigma(\hat D^{(k)},X).(\#eq:step2)
\end{align} 
This ALS approach to MDS was in the air since the early
(unsuccessful) attempts around 1968 of Young and De Leeuw to combine
Torgerson's classic metric MDS method with Kruskal's monotone regression
transformation. All previous implementations of non-metric smacof
use the two-phase approach, and we will do the same in this paper.

As formulated, however, there are some problems with the ALS algorithm.
Step \@ref(eq:step1) is easy to carry out, using monotone regression.
Step \@ref(eq:step2) means solving a metric scaling problem, which is an
iterative proces that requires an infinite number of iterations. Thus,
in the usual implementations, step \@ref(eq:step1) is combined with one
of more iterations of a convergent iterative procedure for metric MDS,
such as smacof. If we take only one of these *inner iterations* the
algorithm becomes indistinguishable from Kruskal's single-phase method.
This has also created much confusion in the past.

In the usual implementations of the ALS approach we solve the first
subproblem \@ref(eq:step1) exactly, while we take only a single step
towards the solution for given $\hat D$ in the second phase
\@ref(eq:step2). If we have an infinite iterative procedure to compute
the optimal $\hat D\in\mathfrak{D}$ for given $X$, then a more balanced
approach would be to take several inner iterations in the first phase and
several inner iterations in the second phase. How many of each, nobody
knows. In our current implementation of smacof we take several inner
iteration steps in the first phase and a single inner iteration step
in the second phase.

## Spline Basis Details

ninner $m$,
degree $k$,
order $d=k+1$,
nknots $m + 2d$,
span $p=d+m$

B-splines

$B_{i,k}(x)$ is zero outside $[t_i,t_{i+k+1}]$

```{r bsplines}
inner = c(.1, .5, .55, .9)
x = 0:10/10
print(x)
for (degree in 0:4) {
  ord <- degree + 1
  knots <- c(rep(0, ord), inner, rep(1, ord))
  a <- splineDesign(knots = knots, ord = ord, x = x)
  for (i in 1:11) {
    cat(formatC(x[i], digits = 2, format = "f"), " *** ", 
        formatC(a[i, ], digits = 4, format = "f"), "\n")
  }
  cat("\n\n")
}
```
```{r kable, echo = FALSE}
x <- matrix(c(0:4, 1:5, rep(4,5), c(6, 8, 10, 12, 14), 5:9), 5, 5)
kable(x, format = "pipe", digits = 0, col.names = c("degree", "order", "ninner", "nknots", "span"), align = 'c')
```

$$
\sum_i B_{i,k}(x)=1
$$



M-splines

$$
M_{i,k}(x)=\frac{k+1}{t_{i+k+1}-t_i}B_{i,k}(X)
$$
then 
$$
\int M_{i,k}(x)dx=1
$$

I-splines
$$
I_{i,k+1}(z)=\int_{-\infty}^zM_{i,k}(x)dx
$$

When is a B-spline increasing ?
$$
\mathcal{D}B_{i,k}(x)=
$$
Thus if
$$
\mathcal{D}\sum_{i=1}^{d+m}\alpha_iB_{i,k}(x)=
$$
It is sufficient that $\alpha_i\leq\alpha_{i+1}$

# Background

## Splines

## Cyclic Coordinate Descent

In the non-linear least squares (NNLS) problem the data are an $n\times p$ matrix
$X$, a vector $y$ with $n$ elements, and a positive semi-definite diagonal matrix
$W$. We want to minimize
$$
\sigma(\beta):=\frac12(X\beta-y)'W(X\beta-y)
$$
over $\beta\geq 0$. In data analysis and statistics the problem is often solved by 
*active set methods*, implemented in R for example by NNLS (@mullen_vanstokkum_23) and FNNLS (@bro_dejong_97). Active set methods are finitely convergent dual methods. While iterating the intermediate solutions are not feasible (i.e. non-negative). In fact 
in dual methods we reach feasibility and optimality at the same time. Also the number of iterations, although theoretically finite, can be very large. 

In each smacof iteration we need an NNLS solution. Especially in the early iterations the solution does not have to be very precise. Also the solution from the previous
NNLS problem will generally provide a very good starting value for the next iteration
(each NNLS problem has a "hot start"). And finally, we would like all internediate solutions to be feasible. These considerations have lead us to using
*cyclic coordinate descent* (CCD).

Suppose the current best feasible solution in CCD iteration $k$ is $\beta^{(k)}$. 
The next CCD iteration changes each of the $p$ coordinates of $\beta^{(k)}$ in turn, maintaining feasibility, while keeping the other $p-1$ coordinates fixed at their current values. Thus within a CCD iteration $k$ we create intermediate solutions $\beta^{(k,1)},\cdots,\beta^{(k,p)}$, where each of the intermediate solutions $\beta^{(k,r)}$ differs from the previous one $\beta^{(k,r-1)}$ in a single coordinate.
For consistency we define $\beta^{(k,0)}:=\beta^{(k)}$. After the iteration is finished
we set $\beta^{(k+1)}=\beta^{(k,p)}$. 

Note that in smacof each iteration modifies the coordinates in the order $1,\cdots,p$, which explains why the method is called "cyclic". There are variations of CCD in which the order within an iteration is random or greedy (choose the coordinate which gives the largest improvement) or zig-zag $1,\cdots,p,p-1,\cdots,1$. We have not tried out these alternatives in  smacf, but we may in the future.

The effect of changing a single coordinate on the loss function is
$$
\sigma(\beta+\epsilon e_j)=\sigma(\beta)+\epsilon\ g_j(\beta)+\frac12\epsilon^2s_{jj},
$$
where $e_j$ is the unit vector corresponding with the coordinate we are changing,
$g(\beta):=\mathcal{D}\sigma(\beta)=X'Wr(\beta)$ is the gradient at $\beta$, and $r(\beta):=X\beta-y$ is the residual. Also $S:=X'WX$. Note that if $s_{jj}=0$
then also $g_j(\beta)=0$ and thus $\sigma(\beta+\epsilon e_j)=\sigma(\beta)$.
In each CCD cycle we simply skip updating coordinate $j$.

If $s_{jj}>0$ then $\sigma(\beta+\epsilon e_j)$ is a strictly convex quadratic
in $\epsilon$, which we must minimize under the constraint $\beta_j+\epsilon\geq 0$ 
or $\epsilon\geq-\beta_j$. Define $\hat\epsilon$ to be the solution of this constrained minimization problem.

The quadratic ... has its minimum at
$$
\tilde\epsilon=-\frac{g_j(\beta)}{s_{jj}}
$$
If $\beta+\tilde\epsilon$ is feasible then it is the update we are looking for. Thus $\hat\epsilon=\tilde\epsilon$. If $\beta+\tilde\epsilon<0$ then the contrained minimum is attained at the boundary, i.e. $\hat\epsilon=-\beta_j$ and the updated $\beta_j$ is zero. Thus, in summary, $\hat\epsilon=\max(\tilde\epsilon,-\beta_j)$.



One of the nice things about CCD is that 
$$
r(\hat\beta)=r(\beta)+\hat\epsilon x_j
$$
$$
g(\hat\beta)=g(\beta)+\hat\epsilon s_j
$$

It follows that $\hat\epsilon=0$ if and only if either $\beta_j=0$ and $g_j(\beta)\geq 0$ or if $g_j(beta)=0$ and $beta_j>0$.

If $g_j(\beta)<0$ then $\tilde\epsilon>0$, and thus $\hat\epsilon>0$ and $\sigma(\hat\beta)<\sigma(\beta)$. Thus we must have $g_j(\beta)\geq 0$.

If $\beta_j>0$ and $g_j(\beta)\not=0$ then there is an $\epsilon$ such that
$\sigma(\beta+\epsilon e_j)<\sigma(\beta)$. Thus if $\beta_j>0$ we must have
$g_j(\beta)=0$.

In summary at the minimum of $\sigma$ over $\beta\geq 0$ 
we must have $\beta_j\geq 0$, $g_j(\beta)\geq 0$, and
$\beta_jg_j(\beta)=0$ for all $j$ (*complementary slackness*).


$$
\sigma(\beta+\epsilon e_j)=\sigma(\beta)+\epsilon\ g_j(\beta)+\frac12\epsilon^2s_{jj},
$$
where $S:=X'WX$.

Now suppose we minimize $\sigma$ over $\beta\geq 0$. 

Our best solution so far is
$\beta^{(k)}\geq 0$. Minimize $\sigma(\beta^{(k)}+\epsilon e_1)$ over $\epsilon$
on the condition that $\beta^{(k)}_1+\epsilon\geq 0$ or $\epsilon\leq-\beta^{(k)}_1$.
If $s_{11}=0$ then also $g_1(\beta)=0$ and we set $\beta^{(k+1,1)}=\beta^{(k,1)}$.
If $s_{11}>0$ we compute 
$$
\tilde\epsilon=-g_1(\beta)/s_{11}
$$
If
$$
\beta^{(k)}_1+\tilde\epsilon\geq 0
$$
then 
$$
\beta^{(k+1,1)}=\beta^{(k)}_1+\tilde\epsilon
$$
If
$$
\beta^{(k)}_1+\tilde\epsilon<0
$$
we set 
$$
\beta^{(k+1,1)}=0.
$$


## Majorization

Majorization, more recently better known as MM (Lange), minimize 

Minimize $\sigma$ over $x\in S$. Suppose there is a function 
$\eta$ on $S\otimes S$ such that

1. $\sigma(x)\leq\eta(x,y)$ for all $x,y\in S$.
2. $\sigma(x)=\eta(x,x)$ for all $x\in S$.

The function $\eta$ is called a *majorization scheme* for $\sigma$ on $S$. A
majorization scheme is *strict* if $\sigma(x)\leq\eta(x,y)$ for all $x,y\in S$
withj $x\not=y$.


Define 

$$
x^{(k+1)}=\mathop{\text{argmin}}_{x\in S}\eta(x,x^{(k)})
$$
assuming that $\eta(x,y)$ attains its minimum over $x\in S$ for each $y$. Then 

1. $\sigma(x^{(k+1)})\leq\eta(x^{(k+1)},x^{(k)})$ by .... 
2. $\eta(x^{(k+1)},x^{(k)})\leq\eta(x^{(k)},x^{(k)})$ by ...
3. $\eta(x^{(k)},x^{(k)})=\sigma(x^{(k)})$ by ...

a. If the minimum in ... is attained for a unique $x$ then
$\eta(x^{(k+1)},x^{(k)})<\eta(x^{(k)},x^{(k)})$
b. If the majorization is strict then
$\sigma(x^{(k+1)})<\eta(x^{(k+1)},x^{(k)})$

## Example

We give a small example in which we minimize $\sigma$ with $\sigma(x)=\sqrt{x}-\log{x}$
over $x>0$. Solving $\mathcal{D}\sigma(x)=0$ gives $x=4$ as the solution we 
are looking for. 

To arrive at this solution using majorization we start with
$$
\sqrt{x}\leq\sqrt{y}+\frac12\frac{x-y}{\sqrt{y}},
$$
which is true because a differentiable concave function such as the square root is majorized by its tangent everywhere.
Inequality ... implies
$$
\sigma(x)\leq\eta(x,y)=\sqrt{y}+\frac12\frac{x-y}{\sqrt{y}}-\log{x}.
$$
Now $\mathcal{D}_1\eta(x,y)=0$ if and only if $x=2\sqrt{y}$ and thus the majorization algorithm is
$$
x^{(k+1)}=2\sqrt{x^{(k)}}
$$
The sequence $x^{(k)}$ converges monotonically to the fixed point $x=2\sqrt{x}$, i.e. to $x=4$. If $x^{(0)}<4$ the sequence is increasing, if $x^{(0)}<4$ it is decreasing. Also, by l'Hôpital,
$$
\lim_{x\rightarrow 4}\frac{2\sqrt{x}-4}{x-4}=\frac12
$$
and thus convergence to the minimizer is linear with asymptotic convergence rate $\frac12$. By another application of l'Hôpital
$$
\lim_{x\rightarrow 4}\frac{\sigma(2\sqrt{x)})-\sigma(4)}{\sigma(x)-\sigma(4)}=\frac14,
$$
and convergence to the minimum is linear with asymptotic convergence rate $\frac14$.
Linear convergence to the minimizer is typical for majorization algorithms, as is
the twice-as-fast linear convergence to the minimum value.

In table ... we show convergence starting at $x=1.5$. 
```{r majiter, echo = FALSE}
x <- 1.5
f <- sqrt(x) - log(x)
f0 <- 2 - log(4)
for (i in  1:15) {
  cat("itel ", formatC(i, digits = 0, width = 2, format = "d"),
      formatC(4 - x, digits = 10, format = "f"),
      formatC(f - f0, digits = 10, format = "f"),
      "\n")
  x <- 2 * sqrt(x)
  f <- sqrt(x) - log(x)
}
```  

```{r majplot, fig.align = "center", echo = FALSE}
x <- 100:500/100
y <- sqrt(x) - log(x)
plot(x, y, type = "l", lwd = 3, col = "RED")
g <- function(x, y) {
return(sqrt(y)+ (x - y) / (2 * sqrt(y)) - log(x))
}
x1 <- 1.5
abline(v = x1)
z1 <- g(x, x1)
lines(x, z1, col = "BLUE")
x2 <- 2 * sqrt(x1)
abline(v = x2)
z2 <- g(x, x2)
lines(x, z2, col = "BLUE")
x3 <- 2 * sqrt(x2)
abline(v = x3)
z3 <- g(x, x3)
lines(x, z3, col = "BLUE")
```

# References


